{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lUcPIO6rI70"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s03K5iu6xAIy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import TextVectorization\n",
        "from keras import layers\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SURCvO3dxkAg"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk7dBSrPQQuI",
        "outputId": "a6493ebd-6647-4bc5-b569-5f65110affdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "np.random.seed(44)\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "30dJve_kMceN"
      },
      "outputs": [],
      "source": [
        "colab = 'colab'\n",
        "local = 'local'\n",
        "env = colab\n",
        "#env = local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3T3jkSyryS2",
        "outputId": "1c7188f1-3cfd-4deb-d584-8b33d66a3c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if env == colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  data_path = '/content/drive/My Drive/eece5644_final_project/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IB0Zt5Vrh_zz"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(data_path + 'tamil_movie_reviews_train.csv')\n",
        "test_df = pd.read_csv(data_path + 'tamil_movie_reviews_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u6-cdFGtvjmR"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop_duplicates().reset_index(drop=True)\n",
        "test_df = test_df.drop_duplicates().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6DpTiF6FPp"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J7A0mafwm_rI"
      },
      "outputs": [],
      "source": [
        "#binary split\n",
        "def binary_split(y):\n",
        "  mid_point = 7\n",
        "  y_mini = y.copy()\n",
        "  for i in range(len(y_mini)):\n",
        "    if y[i] <= mid_point:\n",
        "      y_mini[i] = 0\n",
        "    else:\n",
        "      y_mini[i] = 1\n",
        "  return y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OqYUIjDGgXyp"
      },
      "outputs": [],
      "source": [
        "def tertiary_split(y):\n",
        "  lower_bound = 6\n",
        "  upper_bound = 8\n",
        "  y_mini = y.copy()\n",
        "  for i in range(len(y_mini)):\n",
        "    if y[i] <= lower_bound:\n",
        "      y_mini[i] = 0\n",
        "    elif lower_bound < y[i] <= upper_bound:\n",
        "      y_mini[i] = 1\n",
        "    elif upper_bound < y[i]:\n",
        "      y_mini[i] = 2\n",
        "  return y_mini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def polarize(x, y):\n",
        "  lower_bound = 6\n",
        "  upper_bound = 8\n",
        "  y_mini = y.copy()\n",
        "  x_new = np.array([])\n",
        "  y_mini_new = np.array([])\n",
        "  for i in range(len(y_mini)):\n",
        "    if y[i] <= lower_bound:\n",
        "      x_new = np.append(x_new, x[i])\n",
        "      y_mini_new = np.append(y_mini_new, 0)\n",
        "    elif upper_bound < y[i]:\n",
        "      x_new = np.append(x_new, x[i])\n",
        "      y_mini_new = np.append(y_mini_new, 1)\n",
        "  return x_new, y_mini_new"
      ],
      "metadata": {
        "id": "NEfLed97Pk5l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dpGHpjcsoN3D"
      },
      "outputs": [],
      "source": [
        "# Ratings are given in increments of 0.25, so let's just normalize them\n",
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "df.Rating = df.Rating * 4 - 4\n",
        "df.Rating = df.Rating.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JiqGssNvRyky"
      },
      "outputs": [],
      "source": [
        "# Cite ChatGPT\n",
        "def read_stop_words(filename):\n",
        "    stop_words = []\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            stop_words.append(line.strip())\n",
        "    return stop_words\n",
        "\n",
        "filename = data_path + \"TamilStopWords.txt\"\n",
        "stop_words_list = read_stop_words(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WE3xI8O7R_Zw"
      },
      "outputs": [],
      "source": [
        "def create_vectorizer(stop_words):\n",
        "  return TfidfVectorizer(stop_words = stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NSMxG75GSQQC"
      },
      "outputs": [],
      "source": [
        "def vectorize_tamil_data():\n",
        "  filename = data_path + \"TamilStopWords.txt\"\n",
        "  stop_words_list = read_stop_words(filename)\n",
        "\n",
        "  vectorizer = create_vectorizer(stop_words = stop_words_list)\n",
        "\n",
        "  X = vectorizer.fit_transform(df.ReviewInTamil).toarray()\n",
        "  y = df.Rating\n",
        "\n",
        "  return (X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W2yLgb9tSkWb"
      },
      "outputs": [],
      "source": [
        "def vectorize_english_data():\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jU1E6w-unDFM"
      },
      "outputs": [],
      "source": [
        "vectorizer = create_vectorizer(stop_words = 'english')\n",
        "\n",
        "# vectorizer = TfidfVectorizer(stop_words = 'english')\n",
        "#vectorizer = TextVectorization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DCNHc48aljlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7318b7de-c643-4be8-b281-ba7f358cc33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['அங', 'அட', 'அத', 'அதன', 'அதற', 'அந', 'அன', 'அல', 'அவ', 'அவன', 'அவர', 'அவரத', 'அவள', 'ஆக', 'இங', 'இடத', 'இடம', 'இத', 'இதன', 'இதற', 'இந', 'இன', 'இப', 'இர', 'இவ', 'இவர', 'உன', 'உள', 'எந', 'எனக', 'எனப', 'எனவ', 'எல', 'ஏன', 'ஒர', 'ஓர', 'கள', 'கவ', 'சற', 'தக', 'தத', 'தன', 'தனத', 'தப', 'தல', 'தவ', 'னர', 'பட', 'பத', 'பற', 'பலர', 'மட', 'மற', 'றக', 'லத', 'ளத', 'ளன', 'வந', 'வர'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X, y = vectorize_tamil_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = X.shape[1]\n",
        "print(\"Total number of features:\", num_features)"
      ],
      "metadata": {
        "id": "yGuH8SzNeDGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57324b1-aab5-40ad-c0c8-e1a9a89498c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of features: 2589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "06grt2eIlXaM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "p9GzuPk5YOKo"
      },
      "outputs": [],
      "source": [
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvb8g6dIY9FD"
      },
      "source": [
        "# Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PC1Z3o5JomEv"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix, precision_recall_curve, precision_recall_fscore_support, roc_auc_score, roc_curve, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import time\n",
        "\n",
        "def evaluate_model(y_true, x_test, clf, round=False, dim=3):\n",
        "  '''\n",
        "  y_true: true class labels for test data\n",
        "  x_test: test data features\n",
        "  clf: model to evaluate\n",
        "  '''\n",
        "  y_pred = clf.predict(x_test)\n",
        "  if(round):\n",
        "    y_pred = np.round(y_pred)\n",
        "  report = classification_report(y_true, y_pred, output_dict=True)\n",
        "  print(classification_report(y_true, y_pred))\n",
        "  ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
        "  for c in range(0, dim):\n",
        "    #print(f'Class: {c}')\n",
        "    # precision = report[str(c)]['precision']\n",
        "    # recall = report[str(c)]['recall']\n",
        "    predictions = []\n",
        "    gold = []\n",
        "    for i in y_pred:\n",
        "      if i == c:\n",
        "        predictions.append(0)\n",
        "      else:\n",
        "        predictions.append(1)\n",
        "    for i in y_true:\n",
        "      if i == c:\n",
        "        gold.append(0)\n",
        "      else:\n",
        "        gold.append(1)\n",
        "    PrecisionRecallDisplay.from_predictions(gold, predictions, name = f'Precison/Recall, class {str(c)}')\n",
        "    RocCurveDisplay.from_predictions(gold, predictions, name = f'ROC, class {str(c)}')\n",
        "    inference_times = []\n",
        "  for sample in x_test:\n",
        "    temp_array = [sample]\n",
        "    start_time = time.time()\n",
        "    clf.predict(temp_array)\n",
        "    end_time = time.time()\n",
        "    inference_times.append(end_time - start_time)\n",
        "  fig = px.box(inference_times)\n",
        "  fig.show()\n",
        "\n",
        "def compute_inference_times(y_true, x_test, clf, round=False, dim=3):\n",
        "  '''\n",
        "  y_true: true class labels for test data\n",
        "  x_test: test data features\n",
        "  clf: model to evaluate\n",
        "  '''\n",
        "  inference_times = []\n",
        "  print(x_test)\n",
        "  print(type(x_test))\n",
        "  print(x_test.shape)\n",
        "  for sample in x_test:\n",
        "    temp_array = [sample]\n",
        "    print(temp_array)\n",
        "    print(sample)\n",
        "    print(type(temp_array))\n",
        "    print(type(sample))\n",
        "    start_time = time.time()\n",
        "    clf.predict(temp_array)\n",
        "    end_time = time.time()\n",
        "    inference_times.append(end_time - start_time)\n",
        "  fig = px.box(inference_times)\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct5gxqK228pe"
      },
      "source": [
        "# Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDh_NkBE3AoC"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "y_mini = tertiary_split(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "tertiary_bayes_model = model\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "y_mini = binary_split(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "binary_bayes_model = model\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3VC575VVlet"
      },
      "outputs": [],
      "source": [
        "print(stop_words_list)\n",
        "print(df.ReviewInTamil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5VAAU3qV9sE"
      },
      "source": [
        "# k-NN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGbPtyaHWCYr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# load data from csv files\n",
        "train_data = pd.read_csv(data_path + 'tamil_movie_reviews_train.csv')\n",
        "test_data = pd.read_csv(data_path + 'tamil_movie_reviews_test.csv')\n",
        "\n",
        "# merge train and test data together\n",
        "df = pd.concat([train_data, test_data], ignore_index = True)\n",
        "\n",
        "# assign intelligible rating values for every rating in df\n",
        "min_rating = 1.0\n",
        "max_rating = 4.50\n",
        "alpha = 0.25\n",
        "i = 0\n",
        "\n",
        "# dictionary to map ratings to ints\n",
        "df.Rating = df.Rating * 4 - 4\n",
        "df.Rating = df.Rating.astype(int)\n",
        "\n",
        "# convert text data into numerical features using CountVectorizer\n",
        "X, y = vectorize_tamil_data()\n",
        "\n",
        "## Ternary Model\n",
        "# convert ratings into categorical labels: negative, neutral, and positive\n",
        "neg_bound = 6\n",
        "pos_bound = 8\n",
        "y_categories = y.copy()\n",
        "for i in range(len(y_categories)):\n",
        "    if y_categories[i] <= neg_bound:\n",
        "        y_categories[i] = 'Negative'\n",
        "    elif neg_bound < y_categories[i] <= pos_bound:\n",
        "        y_categories[i] = 'Neutral'\n",
        "    elif y_categories[i] > pos_bound:\n",
        "        y_categories[i] = 'Positive'\n",
        "print(y_categories.value_counts())\n",
        "\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categories, test_size=0.2, random_state = 0)\n",
        "\n",
        "# neighbor values to check\n",
        "neighbors = list(range(1, 51))\n",
        "optimal_accuracy = 0\n",
        "optimal_neighbors = 0\n",
        "\n",
        "# loop through different neighbor values and choose value with highest accuracy\n",
        "for n in neighbors:\n",
        "    knn = KNeighborsClassifier(n_neighbors = n)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if accuracy > optimal_accuracy:\n",
        "        optimal_accuracy = accuracy\n",
        "        optimal_neighbors = n\n",
        "\n",
        "# train KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors = optimal_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "knn_tertiary_model = knn\n",
        "\n",
        "# predict sentiment labels from test data\n",
        "y_pred = knn.predict(X_test)\n",
        "y_pred_series = pd.Series(y_pred)\n",
        "print(y_pred_series.value_counts())\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "## Binary Model\n",
        "# convert ratings into categorical labels: negative, neutral, and positive\n",
        "bound = 7\n",
        "y_categories = y.copy()\n",
        "for i in range(len(y_categories)):\n",
        "    if y_categories[i] <= bound:\n",
        "        y_categories[i] = 'Negative'\n",
        "    elif y_categories[i] > bound:\n",
        "        y_categories[i] = 'Positive'\n",
        "print(y_categories.value_counts())\n",
        "\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categories, test_size=0.2, random_state = 0)\n",
        "\n",
        "# neighbor values to check\n",
        "neighbors = list(range(1, 51))\n",
        "optimal_accuracy = 0\n",
        "optimal_neighbors = 0\n",
        "\n",
        "# loop through different neighbor values and choose value with highest accuracy\n",
        "for n in neighbors:\n",
        "    knn = KNeighborsClassifier(n_neighbors = n)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if accuracy > optimal_accuracy:\n",
        "        optimal_accuracy = accuracy\n",
        "        optimal_neighbors = n\n",
        "\n",
        "# train KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors = optimal_neighbors)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "knn_binary_model = knn\n",
        "\n",
        "\n",
        "# predict sentiment labels from test data\n",
        "y_pred = knn.predict(X_test)\n",
        "y_pred_series = pd.Series(y_pred)\n",
        "print(y_pred_series.value_counts())\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D36OJwTiWAC6"
      },
      "source": [
        "# SVM - Linear Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2kMK-6GWJIm"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Read the data\n",
        "train_df = pd.read_csv(data_path + 'tamil_movie_reviews_train.csv')\n",
        "test_df = pd.read_csv(data_path + 'tamil_movie_reviews_test.csv')\n",
        "\n",
        "# Remove duplicates from the data\n",
        "train_df = train_df.drop_duplicates().reset_index(drop=True)\n",
        "test_df = test_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Ratings are given in increments of 0.25, so let's just normalize them\n",
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "df.Rating = df.Rating * 4 - 4\n",
        "df.Rating = df.Rating.astype(int)\n",
        "\n",
        "# Convert Rating column to integer type\n",
        "df.Rating = df.Rating.astype(int)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "X, y = vectorize_tamil_data()\n",
        "\n",
        "\n",
        "y_mini = tertiary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "# Initialize and train the SVM classifier with Linear kernel\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "tertiary_linear_svm_classifier = svm_classifier\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "y_mini = binary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "# Initialize and train the SVM classifier with Linear kernel\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "binary_linear_svm_classifier = svm_classifier\n",
        "\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM - Polynomial Kernel"
      ],
      "metadata": {
        "id": "kUQrg-yPqPJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#polynomial Kernel\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(44)\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Read the data\n",
        "train_df = pd.read_csv(data_path + 'tamil_movie_reviews_train.csv')\n",
        "test_df = pd.read_csv(data_path + 'tamil_movie_reviews_test.csv')\n",
        "\n",
        "# Remove duplicates from the data\n",
        "train_df = train_df.drop_duplicates().reset_index(drop=True)\n",
        "test_df = test_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Concatenate train and test data for preprocessing\n",
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "df.Rating = df.Rating * 4 - 4\n",
        "df.Rating = df.Rating.astype(int)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "X, y = vectorize_tamil_data()\n",
        "\n",
        "y_mini = tertiary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "\n",
        "# Initialize and train the SVM classifier with polynomial kernel\n",
        "svm_classifier = SVC(kernel='poly', degree=3)  # Set degree to the desired degree of the polynomial\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "tertiary_poly_svm_classifier = svm_classifier\n",
        "\n",
        "y_mini = binary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "# Initialize and train the SVM classifier with Linear kernel\n",
        "svm_classifier = SVC(kernel='poly')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "binary_poly_svm_classifier = svm_classifier\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F3NvpRN-qSFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM - RBF Kernel"
      ],
      "metadata": {
        "id": "xcjKT0kDq-IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RBF Kernel\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(44)\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Read the data\n",
        "train_df = pd.read_csv(data_path + 'tamil_movie_reviews_train.csv')\n",
        "test_df = pd.read_csv(data_path + 'tamil_movie_reviews_test.csv')\n",
        "\n",
        "# Remove duplicates from the data\n",
        "train_df = train_df.drop_duplicates().reset_index(drop=True)\n",
        "test_df = test_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Concatenate train and test data for preprocessing\n",
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "df.Rating = df.Rating * 4 - 4\n",
        "df.Rating = df.Rating.astype(int)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "X, y = vectorize_tamil_data()\n",
        "\n",
        "y_mini = tertiary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "\n",
        "# Initialize and train the SVM classifier with RBF kernel\n",
        "svm_classifier = SVC(kernel='rbf')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "tertiary_rbf_svm_classifier = svm_classifier\n",
        "\n",
        "y_mini = binary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "# Initialize and train the SVM classifier with Linear kernel\n",
        "svm_classifier = SVC(kernel='rbf')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "binary_rbf_svm_classifier = svm_classifier\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZD3zgUqErBUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM - Sigmoid Kernel"
      ],
      "metadata": {
        "id": "KVDShoKvryYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(44)\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Read the data\n",
        "train_df = pd.read_csv(data_path + 'tamil_movie_reviews_train.csv')\n",
        "test_df = pd.read_csv(data_path + 'tamil_movie_reviews_test.csv')\n",
        "\n",
        "# Remove duplicates from the data\n",
        "train_df = train_df.drop_duplicates().reset_index(drop=True)\n",
        "test_df = test_df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Concatenate train and test data for preprocessing\n",
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "df.Rating = df.Rating * 4 - 4\n",
        "df.Rating = df.Rating.astype(int)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "X, y = vectorize_tamil_data()\n",
        "\n",
        "y_mini = tertiary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "\n",
        "# Initialize and train the SVM classifier with Sigmoid kernel\n",
        "svm_classifier = SVC(kernel='sigmoid')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "tertiary_sigmoid_svm_classifier = svm_classifier\n",
        "\n",
        "y_mini = binary_split(y)\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "# Initialize and train the SVM classifier with Linear kernel\n",
        "svm_classifier = SVC(kernel='sigmoid')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "binary_sigmoid_svm_classifier = svm_classifier\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-CynDSfVr3BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4xQV-ymZlQe"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = vectorize_tamil_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20)"
      ],
      "metadata": {
        "id": "s5x6y-bwdJE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpYmJuTCLLHP"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(y)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URRMux8_T0ho"
      },
      "outputs": [],
      "source": [
        "y_mini = tertiary_split(y)\n",
        "y_mini.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdeag6NPT42P"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(y_mini)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q18B8LZQZkt1"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=3)\n",
        "components = pca.fit_transform(X)\n",
        "X_pca = components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJXZZb4xajgD"
      },
      "outputs": [],
      "source": [
        "labels = {\n",
        "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
        "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BepEdYl6bqDA"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(\n",
        "    components, x=0, y=1, z=2, color=y_mini,\n",
        "    title=f'pca',\n",
        "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxJYANw-arCa"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_matrix(\n",
        "    components,\n",
        "    labels=labels,\n",
        "    dimensions=range(0, 3),\n",
        "    color=y_mini\n",
        ")\n",
        "fig.update_traces(diagonal_visible=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster"
      ],
      "metadata": {
        "id": "fmraczfqFN3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_mini, test_size=.20)\n",
        "np.random.seed(40)\n",
        "distortions = []\n",
        "inertias = []\n",
        "mapping1 = {}\n",
        "mapping2 = {}\n",
        "K = range(1, 10)\n",
        "\n",
        "for k in K:\n",
        "    # Building and fitting the model\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(X_train)\n",
        "    kmeanModel.fit(X_train)\n",
        "\n",
        "    distortions.append(sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                        'euclidean'), axis=1)) / X_train.shape[0])\n",
        "    inertias.append(kmeanModel.inertia_)\n",
        "\n",
        "    mapping1[k] = sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                   'euclidean'), axis=1)) / X_train.shape[0]\n",
        "    mapping2[k] = kmeanModel.inertia_"
      ],
      "metadata": {
        "id": "j3Vwuso9FN3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping1.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "CwLWhJM1FN3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method using Distortion')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WXAZ92GvFN3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping2.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "NsWZdqsxFN3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, inertias, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('The Elbow Method using Inertia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d5iSgmLSFN3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of clusters (k)\n",
        "k = 3\n",
        "\n",
        "# Initialize K-means clustering\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# Get the cluster labels and centroids\n",
        "cluster_labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "9_efxGzQFN3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with cluster labels and 3D coordinates\n",
        "df_plot = pd.DataFrame(X_train, columns=['Feature 1', 'Feature 2', 'Feature 3'])\n",
        "df_plot['Cluster'] = cluster_labels.astype(str)  # Convert cluster labels to string for categorical coloring\n",
        "\n",
        "# Create a 3D scatter plot using Plotly Express\n",
        "fig = px.scatter_3d(df_plot, x='Feature 1', y='Feature 2', z='Feature 3', color='Cluster',\n",
        "                    title='K-means Clustering in 3D', opacity=0.7)\n",
        "\n",
        "\n",
        "# Set layout parameters for the 3D plot\n",
        "fig.update_layout(scene=dict(\n",
        "                    xaxis_title='Feature 1',\n",
        "                    yaxis_title='Feature 2',\n",
        "                    zaxis_title='Feature 3'))\n",
        "\n",
        "# Show the 3D plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "U1Tq5aIjFN3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = metrics.accuracy_score(y_train,kmeans.predict(X_train))\n",
        "score"
      ],
      "metadata": {
        "id": "TjJ12dCeFN3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_test, X_test, kmeans)"
      ],
      "metadata": {
        "id": "Fnh4gy6WHSqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAqnwwAMqAn"
      },
      "source": [
        "# PCA (Binary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = vectorize_tamil_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20)"
      ],
      "metadata": {
        "id": "uQgEhDyaMqAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgmD433xMqAq"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(y)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGUfVJ4kMqAr"
      },
      "outputs": [],
      "source": [
        "y_mini = binary_split(y)\n",
        "y_mini.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0u9FcmyMqAs"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(y_mini)\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9kz18AnMqAt"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "components = pca.fit_transform(X)\n",
        "X_pca = components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddAq2jmEMqAt"
      },
      "outputs": [],
      "source": [
        "labels = {\n",
        "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
        "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gJIH5-tMqAu"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    components, x=0, y=1, color=y_mini,\n",
        "    title=f'pca',\n",
        "    labels={'0': 'PC 1', '1': 'PC 2'}\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4_ie96vMqAu"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_matrix(\n",
        "    components,\n",
        "    labels=labels,\n",
        "    dimensions=range(0, 2),\n",
        "    color=y_mini\n",
        ")\n",
        "fig.update_traces(diagonal_visible=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster (Binary)"
      ],
      "metadata": {
        "id": "tHBMpMwdMqAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(40)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_mini, test_size=.20)\n",
        "distortions = []\n",
        "inertias = []\n",
        "mapping1 = {}\n",
        "mapping2 = {}\n",
        "K = range(1, 10)\n",
        "\n",
        "for k in K:\n",
        "    # Building and fitting the model\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(X_train)\n",
        "    kmeanModel.fit(X_train)\n",
        "\n",
        "    distortions.append(sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                        'euclidean'), axis=1)) / X_train.shape[0])\n",
        "    inertias.append(kmeanModel.inertia_)\n",
        "\n",
        "    mapping1[k] = sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                   'euclidean'), axis=1)) / X_train.shape[0]\n",
        "    mapping2[k] = kmeanModel.inertia_"
      ],
      "metadata": {
        "id": "z6ZnDJ8VMqAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping1.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "ff6TTi9yMqAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method using Distortion')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9GHzsTkbMqAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping2.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "JG_9ZPYQMqAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, inertias, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('The Elbow Method using Inertia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xHDn8D5oMqA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of clusters (k)\n",
        "k = 2\n",
        "\n",
        "# Initialize K-means clustering\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# Get the cluster labels and centroids\n",
        "cluster_labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "qXq8j-BWMqA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with cluster labels and 3D coordinates\n",
        "df_plot = pd.DataFrame(X_train, columns=['Feature 1', 'Feature 2'])\n",
        "df_plot['Cluster'] = cluster_labels.astype(str)  # Convert cluster labels to string for categorical coloring\n",
        "\n",
        "# Create a 3D scatter plot using Plotly Express\n",
        "fig = px.scatter(df_plot, x='Feature 1', y='Feature 2', color='Cluster',\n",
        "                    title='K-means Clustering in 3D', opacity=0.7)\n",
        "\n",
        "\n",
        "# Set layout parameters for the 3D plot\n",
        "fig.update_layout(scene=dict(\n",
        "                    xaxis_title='Feature 1',\n",
        "                    yaxis_title='Feature 2'))\n",
        "\n",
        "# Show the 3D plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "eKqT-RUnMqA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = metrics.accuracy_score(y_test,kmeans.predict(X_test))\n",
        "score"
      ],
      "metadata": {
        "id": "vghET0N0MqA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_test, X_test, kmeans)"
      ],
      "metadata": {
        "id": "VxN3yp9mMqA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM | Neural Net"
      ],
      "metadata": {
        "id": "uNQNBne7HPpl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLEzNYhCoboK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYGSY-6lkU5O"
      },
      "outputs": [],
      "source": [
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Vshys96_jc"
      },
      "outputs": [],
      "source": [
        "def save(model):\n",
        "  # Save the model\n",
        "  model.save('/content/drive/My Drive/eece5644_final_project/lstm_model.keras')\n",
        "\n",
        "def load():\n",
        "  return keras.models.load_model('/content/drive/My Drive/eece5644_final_project/lstm_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFvqNmi_5_Gq"
      },
      "outputs": [],
      "source": [
        "def eval(model, x_test, y_test, batch_size=32):\n",
        "  y_pred = model.predict(x_test)\n",
        "  # classification report\n",
        "  report = classification_report(y_test, y_pred)\n",
        "  print(\"Classification Report:\")\n",
        "  print(report)\n",
        "\n",
        "  # confusion matrix\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "  sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah6YeqQKzBEa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code based off of this: https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
        "\n",
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "y_mini = tertiary_split(y)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1), loss=\"categorical_crossentropy\", metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2hKWQW8KbWWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLEASE DON'T RUN COSTS GPU\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\n",
        "model.save('/content/drive/My Drive/eece5644_final_project/tertiary_lstm_model.keras')"
      ],
      "metadata": {
        "id": "3n7eeaYgJJZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code based off of this: https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
        "\n",
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "y_mini = binary_split(y)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "m4irl3BvbsrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLEASE DON'T RUN COSTS GPU\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\n",
        "model.save('/content/drive/My Drive/eece5644_final_project/binary_lstm_model.keras')"
      ],
      "metadata": {
        "id": "ViUkm0brbt88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJtF91HPVUOu"
      },
      "source": [
        "# English Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMaaNV5JVT3I"
      },
      "outputs": [],
      "source": [
        "# https://keras.io/api/datasets/imdb/\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "# Use pad_sequence to standardize sequence length:\n",
        "# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = keras.utils.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GJVpHMYQC1z"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# English Naive Bayes"
      ],
      "metadata": {
        "id": "RVJOztdAcIYT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KMShd8ZUQv1"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "english_bayes_model = model\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TC8GViNVvby"
      },
      "source": [
        "# k-NN on English Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucRx-QYJVx2_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# neighbor values to check\n",
        "neighbors = list(range(1, 51))\n",
        "optimal_accuracy = 0\n",
        "optimal_neighbors = 0\n",
        "\n",
        "# loop through different neighbor values and choose value with highest accuracy\n",
        "for n in neighbors:\n",
        "    knn = KNeighborsClassifier(n_neighbors = n)\n",
        "    knn.fit(x_train, y_train)\n",
        "    y_pred = knn.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if accuracy > optimal_accuracy:\n",
        "        optimal_accuracy = accuracy\n",
        "        optimal_neighbors = n\n",
        "\n",
        "# train KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors = optimal_neighbors)\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "english_knn_classifier = knn\n",
        "\n",
        "# predict sentiment labels from test data\n",
        "y_pred = knn.predict(x_test)\n",
        "y_pred_series = pd.Series(y_pred)\n",
        "print(y_pred_series.value_counts())\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xAnGyMdsFw"
      },
      "source": [
        "# SVM on English Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcUqhe56dtsb"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize and train the SVM classifier with Sigmoid kernel\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(x_test)\n",
        "\n",
        "english_svm_classifier = svm_classifier\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual Label'], colnames=['Predicted Label'])\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vap4DV_VjcA"
      },
      "source": [
        "# LSTM on English Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhN9eAMbVpTg"
      },
      "outputs": [],
      "source": [
        "# CODE TAKEN FROM: https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
        "\n",
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# Add 2 bidirectional LSTMs\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "print(x)\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPghDuCEViPz"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxNezAHlVq6Y"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99luC5fjYyOn"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/My Drive/eece5644_final_project/english_lstm_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZzxmIpgK9Dt"
      },
      "source": [
        "# PCA | English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSoed8zFK9D_"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "# Use pad_sequence to standardize sequence length:\n",
        "# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = keras.utils.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xveKxusDK9EB"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "components = pca.fit_transform(x_train)\n",
        "X_pca = components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y45kiQlnK9EB"
      },
      "outputs": [],
      "source": [
        "labels = {\n",
        "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
        "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wnXKhqnK9EC"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    components, x=0, y=1, color=y_train,\n",
        "    title=f'pca',\n",
        "    labels={'0': 'PC 1', '1': 'PC 2'}\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOcH5kF6K9EC"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_matrix(\n",
        "    components,\n",
        "    labels=labels,\n",
        "    dimensions=range(0, 2),\n",
        "    color=y_train\n",
        ")\n",
        "fig.update_traces(diagonal_visible=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means | English"
      ],
      "metadata": {
        "id": "kDJ3B90SK9ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_train, test_size=.20)\n",
        "np.random.seed(40)\n",
        "distortions = []\n",
        "inertias = []\n",
        "mapping1 = {}\n",
        "mapping2 = {}\n",
        "K = range(1, 10)\n",
        "\n",
        "for k in K:\n",
        "    # Building and fitting the model\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(X_train)\n",
        "    kmeanModel.fit(X_train)\n",
        "\n",
        "    distortions.append(sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                        'euclidean'), axis=1)) / X_train.shape[0])\n",
        "    inertias.append(kmeanModel.inertia_)\n",
        "\n",
        "    mapping1[k] = sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                   'euclidean'), axis=1)) / X_train.shape[0]\n",
        "    mapping2[k] = kmeanModel.inertia_"
      ],
      "metadata": {
        "id": "FzWzq22SK9EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping1.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "sVEPEZK2K9EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method using Distortion')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vHqyEKvFK9EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping2.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "DDI6vA_vK9EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, inertias, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('The Elbow Method using Inertia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2xYCaRApK9EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of clusters (k)\n",
        "k = 2\n",
        "\n",
        "# Initialize K-means clustering\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# Get the cluster labels and centroids\n",
        "cluster_labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "BI-Wm_vWK9EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with cluster labels and 3D coordinates\n",
        "df_plot = pd.DataFrame(X_train, columns=['Feature 1', 'Feature 2'])\n",
        "df_plot['Cluster'] = cluster_labels.astype(str)  # Convert cluster labels to string for categorical coloring\n",
        "\n",
        "# Create a 3D scatter plot using Plotly Express\n",
        "fig = px.scatter(df_plot, x='Feature 1', y='Feature 2', color='Cluster',\n",
        "                    title='K-means Clustering in 3D', opacity=0.7)\n",
        "\n",
        "\n",
        "# Set layout parameters for the 3D plot\n",
        "fig.update_layout(scene=dict(\n",
        "                    xaxis_title='Feature 1',\n",
        "                    yaxis_title='Feature 2'))\n",
        "\n",
        "# Show the 3D plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "zB0nMxxpK9EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = metrics.accuracy_score(y_train,kmeans.predict(X_train))\n",
        "score"
      ],
      "metadata": {
        "id": "lgEHDz8qK9EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_test, X_test, kmeans)"
      ],
      "metadata": {
        "id": "-l2Cq2zIK9EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "o5qwYaz6keeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text data into numerical features using CountVectorizer\n",
        "X, y = vectorize_tamil_data()\n",
        "\n",
        "## Ternary Model\n",
        "# convert ratings into categorical labels: negative, neutral, and positive\n",
        "neg_bound = 6\n",
        "pos_bound = 8\n",
        "y_categories_tertiary = y.copy()\n",
        "for i in range(len(y_categories_tertiary)):\n",
        "    if y_categories_tertiary[i] <= neg_bound:\n",
        "        y_categories_tertiary[i] = 'Negative'\n",
        "    elif neg_bound < y_categories_tertiary[i] <= pos_bound:\n",
        "        y_categories_tertiary[i] = 'Neutral'\n",
        "    elif y_categories_tertiary[i] > pos_bound:\n",
        "        y_categories_tertiary[i] = 'Positive'\n",
        "\n",
        "y_mini = tertiary_split(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "\n",
        "print(\"Evaluating tertiary bayes model\")\n",
        "evaluate_model(y_test, X_test, tertiary_bayes_model)\n",
        "print(\"Evaluating tertiary SVM model\")\n",
        "evaluate_model(y_test, X_test, tertiary_linear_svm_classifier)\n",
        "print(\"Evaluating tertiary kNN model\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categories_tertiary, test_size=.20)\n",
        "evaluate_model(y_test, X_test, knn_tertiary_model)"
      ],
      "metadata": {
        "id": "G3TSF-5ykgP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = vectorize_tamil_data()\n",
        "y_mini = tertiary_split(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "model = keras.models.load_model('/content/drive/My Drive/eece5644_final_project/tertiary_lstm_model.keras')\n",
        "\n",
        "# This generates plots but throws an error if not all classes contain values\n",
        "evaluate_model(y_test, X_test, model, True, dim=3)"
      ],
      "metadata": {
        "id": "YXJwtJ2fCCg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = vectorize_tamil_data()\n",
        "y_mini = binary_split(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "model = keras.models.load_model('/content/drive/My Drive/eece5644_final_project/binary_lstm_model.keras')\n",
        "\n",
        "# This generates plots but throws an error if not all classes contain values\n",
        "evaluate_model(y_test, X_test, model, True, dim=2)"
      ],
      "metadata": {
        "id": "LGYhItAuEQgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = keras.utils.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = keras.models.load_model('/content/drive/My Drive/eece5644_final_project/english_lstm_model.keras')\n",
        "\n",
        "# This generates plots but throws an error if not all classes contain values\n",
        "# evaluate_model(y_test, x_test, model, True, dim=2)\n"
      ],
      "metadata": {
        "id": "TaoeQW42IkqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Binary Model\n",
        "# convert ratings into categorical labels: negative, neutral, and positive\n",
        "bound = 7\n",
        "y_categories_binary = y.copy()\n",
        "for i in range(len(y_categories)):\n",
        "    if y_categories[i] <= bound:\n",
        "        y_categories[i] = 'Negative'\n",
        "    elif y_categories[i] > bound:\n",
        "        y_categories[i] = 'Positive'\n",
        "print(y_categories.value_counts())\n",
        "\n",
        "y_mini = binary_split(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_mini, test_size=.20)\n",
        "\n",
        "\n",
        "\n",
        "evaluate_model(binary_bayes_model)\n",
        "\n",
        "evaluate_model(knn_tertiary_model)\n",
        "evaluate_model(knn_binary_model)\n",
        "\n",
        "evaluate_model(tertiary_linear_svm_classifier)\n",
        "evaluate_model(binary_linear_svm_classifier)\n",
        "\n",
        "evaluate_model(english_bayes_model)\n",
        "evaluate_model(english_knn_classifier)\n",
        "# evaluate_model(english_svm_classifier)"
      ],
      "metadata": {
        "id": "qKQRqr2vBRWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_mini, test_size=.20)\n",
        "np.random.seed(40)\n",
        "distortions = []\n",
        "inertias = []\n",
        "mapping1 = {}\n",
        "mapping2 = {}\n",
        "K = range(1, 10)\n",
        "\n",
        "for k in K:\n",
        "    # Building and fitting the model\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(X_train)\n",
        "    kmeanModel.fit(X_train)\n",
        "\n",
        "    distortions.append(sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                        'euclidean'), axis=1)) / X_train.shape[0])\n",
        "    inertias.append(kmeanModel.inertia_)\n",
        "\n",
        "    mapping1[k] = sum(np.min(cdist(X_train, kmeanModel.cluster_centers_,\n",
        "                                   'euclidean'), axis=1)) / X_train.shape[0]\n",
        "    mapping2[k] = kmeanModel.inertia_"
      ],
      "metadata": {
        "id": "7rfhOIO5K4Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping1.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "J4EdwiulK4Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method using Distortion')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mng9N6roK4Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in mapping2.items():\n",
        "    print(f'{key} : {val}')"
      ],
      "metadata": {
        "id": "h6KtA-KIK4Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, inertias, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('The Elbow Method using Inertia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_3EEurpEK4Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of clusters (k)\n",
        "k = 3\n",
        "\n",
        "# Initialize K-means clustering\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# Get the cluster labels and centroids\n",
        "cluster_labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "SFvYeqOwK4Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with cluster labels and 3D coordinates\n",
        "df_plot = pd.DataFrame(X_train, columns=['Feature 1', 'Feature 2', 'Feature 3'])\n",
        "df_plot['Cluster'] = cluster_labels.astype(str)  # Convert cluster labels to string for categorical coloring\n",
        "\n",
        "# Create a 3D scatter plot using Plotly Express\n",
        "fig = px.scatter_3d(df_plot, x='Feature 1', y='Feature 2', z='Feature 3', color='Cluster',\n",
        "                    title='K-means Clustering in 3D', opacity=0.7)\n",
        "\n",
        "\n",
        "# Set layout parameters for the 3D plot\n",
        "fig.update_layout(scene=dict(\n",
        "                    xaxis_title='Feature 1',\n",
        "                    yaxis_title='Feature 2',\n",
        "                    zaxis_title='Feature 3'))\n",
        "\n",
        "# Show the 3D plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "Hj7Bl6eSK4Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = metrics.accuracy_score(y_train,kmeans.predict(X_train))\n",
        "score"
      ],
      "metadata": {
        "id": "Qxj7CixRK4Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(y_test, X_test, kmeans)"
      ],
      "metadata": {
        "id": "mHipjk8HK4Vs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1lUcPIO6rI70",
        "SURCvO3dxkAg",
        "GM6DpTiF6FPp",
        "Xvb8g6dIY9FD",
        "Ct5gxqK228pe",
        "V5VAAU3qV9sE",
        "D36OJwTiWAC6",
        "kUQrg-yPqPJk",
        "xcjKT0kDq-IG",
        "KVDShoKvryYB",
        "h4xQV-ymZlQe",
        "fmraczfqFN3W",
        "wXAqnwwAMqAn",
        "tHBMpMwdMqAv",
        "uNQNBne7HPpl",
        "YJtF91HPVUOu",
        "RVJOztdAcIYT",
        "4TC8GViNVvby",
        "G-xAnGyMdsFw",
        "2vap4DV_VjcA",
        "LZzxmIpgK9Dt",
        "kDJ3B90SK9ED"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}